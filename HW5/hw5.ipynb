{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szZp0zKGJ_y3",
        "colab_type": "text"
      },
      "source": [
        "# **1. General Concepts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3evfaysLvroL",
        "colab_type": "text"
      },
      "source": [
        "### **What is Artificial Intelligence?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7vRN4Q9vuVz",
        "colab_type": "text"
      },
      "source": [
        "To understand the influence and popularity of artificial intelligence, one must first look back in history. Since ancient times, people have tried bringing \"life\" to machines, giving them some form of independence and individuality. This manifested in \"automatons,\" machines that capable of performing a programmed set of instructions. Without a person's control, these machines were able to perform given jobs (albeit for small, minimal tasks). \n",
        "\n",
        "An example of automatons: https://youtu.be/uzM32wVTbsY\n",
        "\n",
        "There were also myths of statues and machines coming to life, acting on their own will and being able to feel. Examples of this include the Greek stories of the Argonaunts' experience against the giant machine Talos, or Pygmalion falling in love with the statue Galatea that would be granted life by Aphrodite. Long before modern civilization the idea of bringing human qualities to objects existed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLOz1OCKpRu",
        "colab_type": "text"
      },
      "source": [
        "  With the advent of the digital computer came the field of artificial intelligence. To understand what \"artificial intelligence\" is, one must first think what \"intelligence\" means in general. View the following two examples\n",
        "\n",
        "  > A computer and a young child being given the task of adding two numbers together.\n",
        "\n",
        "  > The computer and a young child playing a game of checkers. \n",
        "\n",
        "  For the first example, would one necessarily say the two show signs of intelligence? For both, they be given the answer \"no\" (although there may be a slightly more mixed answer to the child). \n",
        "\n",
        "  However, note the second example. While both subjects are still performing a relatively simple job, there is a lot more factors and biases that fall into play. Overall, this example needs a lot more decision making than the first. This is where artificial intelligence becomes involved. It is allowing computers to essentially \"act\" like humans in our decision making abilities.\n",
        "\n",
        "  Now, by giving computers the ability to perform intensive decision making operations, what benefit do we gain? What jobs can be done? Doing so allows computers to perform tasks that range from image classification to stock prediction to driving cars. These are problems that we cannot program a computer to solve through a deterministic approach, as there are far too many variables and cases to factor in. We are essentially trying to program computers in such a way that they are able to perform non-deterministic jobs, ones that rely on heuristics instead of absolute certainty. \n",
        "\n",
        "Over the years there have been many different ways that artificial intelligence have been studies and attempted. Some of these methods include computational neuroscience, symbol recognition, and statistical reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4MCAlUp0wlb",
        "colab_type": "text"
      },
      "source": [
        "### **What is Machine Learning?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqX3qeMTKvM7",
        "colab_type": "text"
      },
      "source": [
        "Machine learning (ML) can be considered a subset of artificial intelligence (AI); all ML is AI while not all AI is ML. Machine learning takes our idea of programming and completely revamps it. Instead of giving our computer some form of input and a programmed set of instructions, we rely on the computer essentially \"building itself\" the program. Instead of necessarily programming the specific actions for the computer to do to solve the problem, we instead program a means for it interpret given data and perform a specific action based on it. Examples of these include decision trees and neural networks. \n",
        "\n",
        "The ending goal of machine learning is what our ancestors dreamed of, allowing a machine to act and perform on its own, not needing to specifically follow a preprogrammed set of instructions. However, this again begs the question of if this is truly artificial intelligence. Despite being self-taught, it relied on a large amount of data being fed to it. Also, the algorithms and overall model of the program that allowed it to be able to learn was built by humans. So could one say that we merely programmed it to follow rules that would allow it to learn? Thereby still leaving the machine as merely...a machine. A cold shell incapable of anything other than following orders. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DDYwIx-0zNn",
        "colab_type": "text"
      },
      "source": [
        "### **What is Deep Learning?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU_xb4hIKzib",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Deep learning is a form of machine learning. It relies on the use of neural networks, a simuation of our brain's neurons. Inside our brain, there are billions upon billions of neurons, which are nerve that connect to each other through axons and dendrites. When a neuron \"fires,\" it sends a signal along it's axon to the receiving neuron's dendrite. This results in a chain reaction of the receiving node firing off and repeating the process. A neural network works in a similar fashion. There are multiple neurons connected to each other in layers, and each connecting edge has a specific weight to it. For each neuron, a function occurs based on the input given to it, with it's output being passed to the next layer. \n",
        "\n",
        "Through this process, we are able to end up with a \"self-taught\" program. We give in a large amount of data, usually on the magnitude of hundreds to thousands, and allow the program to incorrectly guess until it is able to do so correctly. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUnUQ329KJhu",
        "colab_type": "text"
      },
      "source": [
        "# **2. Basic Concepts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2069gOzAvFP",
        "colab_type": "text"
      },
      "source": [
        "### **Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdQKc7iFDq69",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is one way we form relationships between data. Given a set of inputs and their respective outputs, the goal of linear regression is to find a line that best models the graphed data. The formula used for representing our line is of the form\n",
        "\n",
        "$\\hat{y}= b + w_0 + \\sum_{i=1}^{n} w_ix_i$\n",
        "\n",
        "This is merely a modified version of the line equation, $y = mx + b$, where\n",
        "\n",
        "$w_0$ is what we call a bias term. It is a constant, 1, that we give the model to help better fit the data as the activation functions occur. \n",
        "\n",
        "$w_i$ is a set of weights that we use to create a line out of our given data by multiplying our x-values with, similar to m in the line equation. \n",
        "\n",
        "$x_i$ is our set of inputs\n",
        "\n",
        "$b$ is how much we we need to vertically shift the line\n",
        "\n",
        "Through linear regression, we will end up with a resulting line that should be fairly accurate. This means that if we end up giving an input that was not in our initial training dataset, the resulting line would be able to give a decently accurate guess on the resulting output. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_dQBKcEAzre",
        "colab_type": "text"
      },
      "source": [
        "### **Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPofcYuCQhIw",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression is another way we form relationships with data. While linear regression was based around trying to form a linear relationship between our input and output sets in order to allow the program to guess an answer on future inputs, logistic regression deals in discrete values. This is useful for classification problems, where you try to group pieces of data together based on some characteristic. \n",
        "\n",
        "The formula for logistic regression is of the form\n",
        "\n",
        "$P=\\frac{1}{1+e^{-(w_0 + \\sum_{i=1}^{n}w_ix_i)}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfsjBu1ckUC6",
        "colab_type": "text"
      },
      "source": [
        "### **Gradient**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZCvCgk2kXxl",
        "colab_type": "text"
      },
      "source": [
        "Gradient is a vector of a function's growth over time, giving us a direction to the point of greatest increase. It can be thought of as the function's partial derivatives for every input variable, or essentially \"how steep is the slope at x point.\" At a local maximum or minimum, gradient would be zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmTQZRiDwRz",
        "colab_type": "text"
      },
      "source": [
        "### **Gradient Descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKpYNbgWZN8R",
        "colab_type": "text"
      },
      "source": [
        "To optimize our regression function, our goal is to minimize the error in our guess from the actual result. This involves trial and error of different weight values so we can the optimal set (which would occur when we found the global maximum of function's gradient). To speed up the process, gradient descent can be used. This algorithm involves making large strides while our current set of weights are very innacurate, and making smaller ones as we get closer and closer to the true values. To do this algorithm, we begin by picking up a random starting value and calculating the loss function after comparing our resulting outputs with the true outputs (this process is known as calculating the Loss Function). Depending on our result, we would move in the direction of the negative gradient. By doing this we would eventually converge to our optimal values. \n",
        "\n",
        "Three popular forms of gradient descent are: batch, mini-batch, and stochastic gradient descent. Batch gradient descent operates on the entire batch of data for each step, taking the error for each input and using their average to calculate the gradient. Mini-batch operates similarly, except on small subsets of data. Stochastic gradient descent relies on randomly choosing a single piece of data and using that to update our gradient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FwQG1EHKQGP",
        "colab_type": "text"
      },
      "source": [
        "# **3. Building a Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqLwH1fJySQx",
        "colab_type": "text"
      },
      "source": [
        "Building a model first consists of preparing two sets of images for your model to use. The first is a set of training data that will be used to actually train the model and have it learn the necessary values needed to properly fit what is given, and the second is known as a validation set. This will be used both during and after training to test the model's current fit with regards to the training set. Usually the initial chunk of data is broken into an 80:20 split for our two sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJxA1eTFzaLy",
        "colab_type": "text"
      },
      "source": [
        "Original source code for snippets: https://github.com/schneider128k/machine_learning_course/blob/master/fashion_items_classification_dense_layers_2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1BdK10Fzm-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "print(\"Train Images Size:\", train_images.shape)\n",
        "print(\"Train Labels Size:\", train_labels.shape)\n",
        "print(\"Test Images Size:\", test_images.shape)\n",
        "print(\"Test Labels Size:\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYfrFExm0p3b",
        "colab_type": "text"
      },
      "source": [
        "Now that we have loaded our dataset, it is time to actually build our model. This involves creating layers of neurons, each with their own activation function. By default, we need at least an input and an output layer, with the hidden layers in between being chosen based on model optimization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6wjfOwH3uX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "      tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "      tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O8dSYLH3tvj",
        "colab_type": "text"
      },
      "source": [
        "To understand some of the terminology in the code above:\n",
        "\n",
        "*   Sequential: Initializes the model as a linear series of neuron layers\n",
        "*   Flatten: Given an input tensor of a specific size, it flattens it into a 1D array\n",
        "*   Dense: A standard dense neural network that consists of a given amount of neurons and performs a given activation function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Afn5iL4xeD",
        "colab_type": "text"
      },
      "source": [
        "Notice that the activation for the two layers are different. This is because each layer can have a different type of function, each of which gives a specific type of output and should be used depending on what type of problem is given. \n",
        "\n",
        "Some specific types of activation functions are:\n",
        "\n",
        "* elu: Returns x if x > 0, alpha*(exp(x)-1) if x , 0\n",
        "* relu: Returns a tensor of max(x, 0) for each element\n",
        "* sigmoid: Returns 1 / 1 + exp(-x)\n",
        "* linear: Returns the given input tensor\n",
        "\n",
        "For more information about activation functions:  https://keras.io/activations/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-lFf7eEKRhr",
        "colab_type": "text"
      },
      "source": [
        "# **4. Compiling a Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDW42SPo7M2D",
        "colab_type": "text"
      },
      "source": [
        "Once we have created our model, it is time to compile our model. In keras, this involves passing in an optimizer and a loss function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBWDDRLB66bC",
        "colab_type": "text"
      },
      "source": [
        "### **Optimizers**\n",
        "\n",
        "Optimizer functions allow us to determine how much the weights of the functions will be changed based on the loss function's results. \n",
        "\n",
        "Some types of optimizer functions include:\n",
        "\n",
        "*   Stochastic Gradient Descent\n",
        "*   Adam\n",
        "*   Adagrad\n",
        "\n",
        "More types of optimizer functions (in keras): https://keras.io/optimizers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYo41QBt65tc",
        "colab_type": "text"
      },
      "source": [
        "### **Loss Functions**\n",
        "\n",
        "Loss Functions essentially tell us how bad our network's guess was. In training, you want this value minimized (however, by minimizing too much you run into overfitting, which will be discussed further on). \n",
        "\n",
        "Some types of loss functions include:\n",
        "\n",
        "*   Mean squared error\n",
        "*   Squared hinge\n",
        "*   Categorical Cross-entropy\n",
        "*   Binary Cross-entropy\n",
        "\n",
        "These functions take in a guess, y, and the learning rate, $\\alpha$. $\\alpha$ controls how slow/fast the model will learn. The goal for choosing $\\alpha$ is to have a value that is not too large (you may jump over the minimum we are trying to find), and not too small (the model will take forever to find the minimum). \n",
        "\n",
        "More type of loss functions (in keras): https://keras.io/losses/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO3JxSFn6yR5",
        "colab_type": "text"
      },
      "source": [
        "The following code snippets an example of the functions \"binary cross-entropy\" and \"mean squared error,\" written in numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4T2JDSV7-jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def binary_cross_entropy(y, a):\n",
        "  return -(y * np.log10(a)) - ((1 - y) * np.log10(1 - a))\n",
        "\n",
        "def mean_squared_error(y, a):\n",
        "  return (1 / 2) * (a - y) * (a - y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LW0Z3j-4AuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile(model):\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHOnMYMGRs4U",
        "colab_type": "text"
      },
      "source": [
        "You can also pass in a metrics function, which is how your model will be judged when it comes to its performance. This is similar to its loss functions, but are not used for model training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YATBvoNPKUIM",
        "colab_type": "text"
      },
      "source": [
        "# **5. Training a Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_m7dNCVSLLJ",
        "colab_type": "text"
      },
      "source": [
        "Now that we have compiled the model, it is time to start training the model and having it fit our data. This involves running through our data and performing gradient descent for a given amount of epochs. The goal for the number of epochs is to fit the data properly, but not too much. We want to avoid the following issues:\n",
        "\n",
        "**Overfitting:**\n",
        "\n",
        "![Overfitting](https://miro.medium.com/max/1000/1*Di7rY6ALXtkhlmlcKRSCoA.png)\n",
        "\n",
        "Overfitting occurs when the model understands the model to such a degree that it is able to essentially mimic the dataset's shape. This is not a good thing to have occur because the model will not be ready to handle outside data. Currently, it is the equivalent of showing a child code. Are they able to copy it down and create an exact copy of the code? Yes. Will they be able to write something else? Will they be able to rewrite their code in such a way that it can work for a similar problem but of different structure? No. \n",
        "\n",
        "\n",
        "**Underfitting:**\n",
        "\n",
        "![Underfitting](https://miro.medium.com/max/1000/1*kZfqaD6hl9iYGYXkMwV-JA.png)\n",
        "\n",
        "\n",
        "Underfitting occurs when your model essentially fails to learn. If we were to graph the data of our inputs and their true outputs, our model was unable to form a similar shape. This can occur if the model was not complex enough for evaluation the function for the dataset. \n",
        "\n",
        "Source for images: https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CC2dsp-fhXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build()\n",
        "model = compile(model)\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-meSYRhKVh4",
        "colab_type": "text"
      },
      "source": [
        "# **6. Finetuning a pretrained model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzY6w0P7l40V",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "  When you are viewing a new problem that requires a model, one should first look to see if a previously made model can be used. Whether it may be yours or an open-source available one. This is because even though the problems needed may be of different topics, the underlying structure and methodology could be shared with other types of models. \n",
        "\n",
        "  For example, one can obtain and finetune a premade convolutional neural network base to work with their given data. To start the process of finetuning, one must first obtain a pre-trained model. With the new model, freeze all of the model's layers so as not to lose the model's pretrained information. Now, a classifier must be designed and attached to the base. Once attached, train the classifier, making sure to avoid overfitting. Overfitting can be prevented by only training the outermost layer of our model. \n",
        "\n",
        "  After one has created and trained a new model, they should be able to use it to solve their initial problem with some degree of success. Depending on how satisfactory that value is, they may go back and try to modify some parameters in the model. This can include variables such as epochs, size of epochs, number of neurons, and learning rate. With pretrained models, one can also try to unfreeze different layers and adding data augmentation to the training dataset. This involves modifying selected data such that it is different in some way from its original state, while still being able to produce the same classification output. For example, in image classification one can resize images or change the color filter. The overall goal of this is to produce a new piece of data that is \"similar, yet different.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVZl5HO-4BZy",
        "colab_type": "text"
      },
      "source": [
        "For original code source, as well as the ability to view a full example of finetuning: https://drive.google.com/file/d/10U3mokqzeJPUNWRYFNMK7ZgjHhlav3K5/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMbnzCFk4UmW",
        "colab_type": "text"
      },
      "source": [
        "## **Example:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKKpYnVJ6aI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import Xception\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ1WKFLf6f0q",
        "colab_type": "text"
      },
      "source": [
        "Here, we chose the Xception convolutional neural network base. You can see I used a pretrained model from keras, in this situation it was Xception.\n",
        "\n",
        "When I create my sequential model, I add my Xception model but then I add a relu activation layer and a sigmoid activation layer.\n",
        "\n",
        "While this does not directly change the conv_base pretrained model it does help to improve our performance as we can add layers specific to our use case - whether we're doing binary classification or a different classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfZ4ldpx4A5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rGqLT7S4mH-",
        "colab_type": "text"
      },
      "source": [
        "Before adding any extra layers, it is important to freeze here as we do not want anything in this section to be modified yet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnHi4xxi4sSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrVnKixd4xlP",
        "colab_type": "text"
      },
      "source": [
        "Now we can add our layers ontop of this base. Here, we chose to create a sequential system with only two layers. The input layer will consist of 256 neurals all triggered by the \"relu\" activation function, which will all be funelled into a 1-neuron output layer with a sigmoid activation function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stUmVYLm42Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Ns9-Uz5R6j",
        "colab_type": "text"
      },
      "source": [
        "Before compiling and training the model, the following snipped shows how data augmentation works. For our training dataset, we will resizing and reshaping the image before passing it to our network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmJHfYsY5Qur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ALE-u95hM-",
        "colab_type": "text"
      },
      "source": [
        "Now we can begin compiling and training as normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp2uihc15i0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer=optimizers.RMSprop(lr=2e-5), \n",
        "    metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2IRe2w5oqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMKhlsD252KE",
        "colab_type": "text"
      },
      "source": [
        "After training, we can begin finetuning. To start, unfreeze a specific amount of layers in our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arm-OKpz54Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'conv2d_4':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzVLLHxK6Avq",
        "colab_type": "text"
      },
      "source": [
        "From there, you can begin retraining the model with new parameters given. Based on results of the model after this, one can go back and repeat the following process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uThXXk06JCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5), \n",
        "    metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}